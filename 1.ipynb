{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "53dc91cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: scikit-learn in ./.venv/lib/python3.12/site-packages (1.8.0)\n",
      "Requirement already satisfied: numpy>=1.24.1 in ./.venv/lib/python3.12/site-packages (from scikit-learn) (2.4.2)\n",
      "Requirement already satisfied: scipy>=1.10.0 in ./.venv/lib/python3.12/site-packages (from scikit-learn) (1.17.0)\n",
      "Requirement already satisfied: joblib>=1.3.0 in ./.venv/lib/python3.12/site-packages (from scikit-learn) (1.5.3)\n",
      "Requirement already satisfied: threadpoolctl>=3.2.0 in ./.venv/lib/python3.12/site-packages (from scikit-learn) (3.6.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: pandas in ./.venv/lib/python3.12/site-packages (3.0.0)\n",
      "Requirement already satisfied: numpy>=1.26.0 in ./.venv/lib/python3.12/site-packages (from pandas) (2.4.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in ./.venv/lib/python3.12/site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: six>=1.5 in ./.venv/lib/python3.12/site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: numpy in ./.venv/lib/python3.12/site-packages (2.4.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install scikit-learn\n",
    "%pip install pandas\n",
    "%pip install numpy\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6f2a2dfe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "Part 1: Fuel Consumption -> Horsepower Prediction\n",
      "============================================================\n",
      "\n",
      "1.1 Load and Inspect Dataset\n",
      "Column names: ['Horse Power', 'Fuel Economy (MPG)']\n",
      "Shape: (100, 2)\n",
      "\n",
      "Summary Statistics:\n",
      "       Horse Power  Fuel Economy (MPG)\n",
      "count   100.000000          100.000000\n",
      "mean    213.676190           23.178501\n",
      "std      62.061726            4.701666\n",
      "min      50.000000           10.000000\n",
      "25%     174.996514           20.439516\n",
      "50%     218.928402           23.143192\n",
      "75%     251.706476           26.089933\n",
      "max     350.000000           35.000000\n",
      "\n",
      "Missing values:\n",
      "Horse Power           0\n",
      "Fuel Economy (MPG)    0\n",
      "dtype: int64\n",
      "\n",
      "Training samples: 70\n",
      "Testing samples: 30\n",
      "All models trained, alpha = 1.0\n",
      "            Model  Train MSE  Train MAE  Train R2   Test MSE  Test MAE  Test R2\n",
      "Linear Regression 357.699180  16.061689  0.906320 318.561087 14.940628 0.912561\n",
      " Ridge Regression 357.700562  16.063735  0.906320 318.806477 14.948544 0.912494\n",
      " Lasso Regression 357.743457  16.073268  0.906309 319.982962 14.985418 0.912171\n",
      "   Poly2 + Linear 350.879731  15.995824  0.908106 331.105434 15.148330 0.909118\n",
      "   Poly3 + Linear 345.108668  15.746762  0.909618 318.404012 14.764973 0.912604\n",
      "    Poly2 + Ridge 350.910169  15.994663  0.908098 332.397825 15.167396 0.908763\n",
      "    Poly3 + Ridge 345.232243  15.780679  0.909585 319.418568 14.792167 0.912326\n",
      "    Poly2 + Lasso 352.879446  16.005019  0.907582 342.711175 15.303737 0.905932\n",
      "    Poly3 + Lasso 346.050142  15.845302  0.909371 321.692287 14.858925 0.911702\n",
      "Best model: Poly3 + Linear with test R2=0.9126\n"
     ]
    }
   ],
   "source": [
    "print(f\"=\"*60)\n",
    "print(f\"Part 1: Fuel Consumption -> Horsepower Prediction\")\n",
    "print(f\"=\"*60)\n",
    "\n",
    "df1 = pd.read_csv('/home/veronica/桌面/Homework1/FuelEconomy.csv')\n",
    "print(f\"\\n1.1 Load and Inspect Dataset\")\n",
    "print(f\"Column names: {list(df1.columns)}\")\n",
    "print(f\"Shape: {df1.shape}\")\n",
    "print(f\"\\nSummary Statistics:\")\n",
    "print(df1.describe())\n",
    "print(f\"\\nMissing values:\")\n",
    "print(df1.isnull().sum())\n",
    "\n",
    "# get X and y, X is fuel economy, y is horsepower\n",
    "X1 = df1[['Fuel Economy (MPG)']].values\n",
    "y1 = df1['Horse Power'].values\n",
    "\n",
    "# 1.2 split 70% train 30% test\n",
    "X1_train, X1_test, y1_train, y1_test = train_test_split(X1, y1, test_size=0.3, random_state=42)\n",
    "print(f\"\\nTraining samples: {len(X1_train)}\")\n",
    "print(f\"Testing samples: {len(X1_test)}\")\n",
    "\n",
    "# 1.3 train all the models\n",
    "\n",
    "# basic linear regression\n",
    "lr1 = LinearRegression()\n",
    "lr1.fit(X1_train, y1_train)\n",
    "\n",
    "# ridge with alpha=1.0, i just use default\n",
    "ridge1 = Ridge(alpha=1.0)\n",
    "ridge1.fit(X1_train, y1_train)\n",
    "\n",
    "# lasso also alpha=1.0\n",
    "lasso1 = Lasso(alpha=1.0)\n",
    "lasso1.fit(X1_train, y1_train)\n",
    "\n",
    "# polynomial degree 2\n",
    "poly2_1 = PolynomialFeatures(degree=2)\n",
    "X1_train_poly2 = poly2_1.fit_transform(X1_train)\n",
    "X1_test_poly2 = poly2_1.transform(X1_test)\n",
    "\n",
    "# polynomial degree 3\n",
    "poly3_1 = PolynomialFeatures(degree=3)\n",
    "X1_train_poly3 = poly3_1.fit_transform(X1_train)\n",
    "X1_test_poly3 = poly3_1.transform(X1_test)\n",
    "\n",
    "# poly2 + linear\n",
    "lr_poly2_1 = LinearRegression()\n",
    "lr_poly2_1.fit(X1_train_poly2, y1_train)\n",
    "\n",
    "# poly3 + linear\n",
    "lr_poly3_1 = LinearRegression()\n",
    "lr_poly3_1.fit(X1_train_poly3, y1_train)\n",
    "\n",
    "# poly2 + ridge\n",
    "ridge_poly2_1 = Ridge(alpha=1.0)\n",
    "ridge_poly2_1.fit(X1_train_poly2, y1_train)\n",
    "\n",
    "# poly3 + ridge\n",
    "ridge_poly3_1 = Ridge(alpha=1.0)\n",
    "ridge_poly3_1.fit(X1_train_poly3, y1_train)\n",
    "\n",
    "# poly2 + lasso\n",
    "lasso_poly2_1 = Lasso(alpha=1.0)\n",
    "lasso_poly2_1.fit(X1_train_poly2, y1_train)\n",
    "\n",
    "# poly3 + lasso\n",
    "lasso_poly3_1 = Lasso(alpha=1.0)\n",
    "lasso_poly3_1.fit(X1_train_poly3, y1_train)\n",
    "\n",
    "print(f\"All models trained, alpha = 1.0\")\n",
    "\n",
    "# 1.4 evaluation - calculate mse mae r2 for each model\n",
    "res1 = []\n",
    "\n",
    "y1_train_pred=lr1.predict(X1_train)\n",
    "y1_test_pred=lr1.predict(X1_test)\n",
    "res1.append(['Linear Regression', mean_squared_error(y1_train, y1_train_pred), mean_absolute_error(y1_train, y1_train_pred), r2_score(y1_train, y1_train_pred), mean_squared_error(y1_test, y1_test_pred), mean_absolute_error(y1_test, y1_test_pred), r2_score(y1_test, y1_test_pred)])\n",
    "\n",
    "y1_train_pred=ridge1.predict(X1_train)\n",
    "y1_test_pred=ridge1.predict(X1_test)\n",
    "res1.append(['Ridge Regression', mean_squared_error(y1_train, y1_train_pred), mean_absolute_error(y1_train, y1_train_pred), r2_score(y1_train, y1_train_pred), mean_squared_error(y1_test, y1_test_pred), mean_absolute_error(y1_test, y1_test_pred), r2_score(y1_test, y1_test_pred)])\n",
    "\n",
    "y1_train_pred=lasso1.predict(X1_train)\n",
    "y1_test_pred=lasso1.predict(X1_test)\n",
    "res1.append(['Lasso Regression', mean_squared_error(y1_train, y1_train_pred), mean_absolute_error(y1_train, y1_train_pred), r2_score(y1_train, y1_train_pred), mean_squared_error(y1_test, y1_test_pred), mean_absolute_error(y1_test, y1_test_pred), r2_score(y1_test, y1_test_pred)])\n",
    "\n",
    "y1_train_pred=lr_poly2_1.predict(X1_train_poly2)\n",
    "y1_test_pred=lr_poly2_1.predict(X1_test_poly2)\n",
    "res1.append(['Poly2 + Linear', mean_squared_error(y1_train, y1_train_pred), mean_absolute_error(y1_train, y1_train_pred), r2_score(y1_train, y1_train_pred), mean_squared_error(y1_test, y1_test_pred), mean_absolute_error(y1_test, y1_test_pred), r2_score(y1_test, y1_test_pred)])\n",
    "\n",
    "y1_train_pred=lr_poly3_1.predict(X1_train_poly3)\n",
    "y1_test_pred=lr_poly3_1.predict(X1_test_poly3)\n",
    "res1.append(['Poly3 + Linear', mean_squared_error(y1_train, y1_train_pred), mean_absolute_error(y1_train, y1_train_pred), r2_score(y1_train, y1_train_pred), mean_squared_error(y1_test, y1_test_pred), mean_absolute_error(y1_test, y1_test_pred), r2_score(y1_test, y1_test_pred)])\n",
    "\n",
    "y1_train_pred=ridge_poly2_1.predict(X1_train_poly2)\n",
    "y1_test_pred=ridge_poly2_1.predict(X1_test_poly2)\n",
    "res1.append(['Poly2 + Ridge', mean_squared_error(y1_train, y1_train_pred), mean_absolute_error(y1_train, y1_train_pred), r2_score(y1_train, y1_train_pred), mean_squared_error(y1_test, y1_test_pred), mean_absolute_error(y1_test, y1_test_pred), r2_score(y1_test, y1_test_pred)])\n",
    "\n",
    "y1_train_pred=ridge_poly3_1.predict(X1_train_poly3)\n",
    "y1_test_pred=ridge_poly3_1.predict(X1_test_poly3)\n",
    "res1.append(['Poly3 + Ridge', mean_squared_error(y1_train, y1_train_pred), mean_absolute_error(y1_train, y1_train_pred), r2_score(y1_train, y1_train_pred), mean_squared_error(y1_test, y1_test_pred), mean_absolute_error(y1_test, y1_test_pred), r2_score(y1_test, y1_test_pred)])\n",
    "\n",
    "y1_train_pred=lasso_poly2_1.predict(X1_train_poly2)\n",
    "y1_test_pred=lasso_poly2_1.predict(X1_test_poly2)\n",
    "res1.append(['Poly2 + Lasso', mean_squared_error(y1_train, y1_train_pred), mean_absolute_error(y1_train, y1_train_pred), r2_score(y1_train, y1_train_pred), mean_squared_error(y1_test, y1_test_pred), mean_absolute_error(y1_test, y1_test_pred), r2_score(y1_test, y1_test_pred)])\n",
    "\n",
    "y1_train_pred=lasso_poly3_1.predict(X1_train_poly3)\n",
    "y1_test_pred=lasso_poly3_1.predict(X1_test_poly3)\n",
    "res1.append(['Poly3 + Lasso', mean_squared_error(y1_train, y1_train_pred), mean_absolute_error(y1_train, y1_train_pred), r2_score(y1_train, y1_train_pred), mean_squared_error(y1_test, y1_test_pred), mean_absolute_error(y1_test, y1_test_pred), r2_score(y1_test, y1_test_pred)])\n",
    "\n",
    "# make a table to show results\n",
    "df_res1 = pd.DataFrame(res1, columns=['Model', 'Train MSE', 'Train MAE', 'Train R2', 'Test MSE', 'Test MAE', 'Test R2'])\n",
    "print(df_res1.to_string(index=False))\n",
    "\n",
    "# 1.5 find best model by test r2\n",
    "best_idx=0\n",
    "best_r2=res1[0][6]\n",
    "i=1\n",
    "while i<len(res1):\n",
    "    if res1[i][6]>best_r2:\n",
    "        best_r2=res1[i][6]\n",
    "        best_idx=i\n",
    "    i+=1\n",
    "print(f\"Best model: {res1[best_idx][0]} with test R2={best_r2:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf74dd59",
   "metadata": {},
   "source": [
    "Poly3 + Linear model has the best test R2 (0.9126), but it is very close to the basic Linear Regression (0.9126 vs 0.9126). Actually they are almost the same.\n",
    "\n",
    "The linear models (Linear, Ridge, Lasso) all perform very well with test R2 around 0.912-0.913. This means the relationship between Fuel Economy and Horsepower is mostly linear. Adding polynomial features does not improve the model much.\n",
    "\n",
    "Ridge and Lasso have similar performance to Linear Regression because this is a simple dataset with only one feature. There is not much overfitting to regularize.\n",
    "\n",
    "The best generalizing model is Poly3 + Linear, but simple Linear Regression works almost as well. I think linear relationship is enough for this data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "926118c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "============================================================\n",
      "Part 2: Weather -> Daily Electricity Consumption Prediction\n",
      "============================================================\n",
      "After dropna shape: (1418, 6)\n",
      "Training: 992, Testing: 426\n",
      "            Model     Train MSE  Train MAE  Train R2      Test MSE   Test MAE  Test R2\n",
      "Linear Regression 272403.396174 384.465016  0.276000 248125.785784 375.404537 0.299333\n",
      " Ridge Regression 272403.396254 384.465310  0.276000 248125.799085 375.404855 0.299333\n",
      " Lasso Regression 272404.552102 384.508177  0.275997 248122.648810 375.439837 0.299342\n",
      "   Poly2 + Linear 264765.769932 379.648753  0.296300 255268.493954 379.039083 0.279163\n",
      "   Poly3 + Linear 259249.534870 375.952901  0.310961 265623.657673 385.235167 0.249922\n",
      "    Poly2 + Ridge 264766.578998 379.585166  0.296298 254955.169280 378.881442 0.280048\n",
      "    Poly3 + Ridge 259307.759576 376.033550  0.310806 257659.254512 383.568232 0.272412\n",
      "    Poly2 + Lasso 264881.385611 379.509069  0.295993 253247.451059 378.033190 0.284870\n",
      "    Poly3 + Lasso 261405.827082 375.970982  0.305230 249726.476276 376.697061 0.294813\n",
      "Best model: Lasso Regression with test R2=0.2993\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/veronica/桌面/Homework1/.venv/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:716: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.230e+08, tolerance: 3.732e+04\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/veronica/桌面/Homework1/.venv/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:716: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.299e+08, tolerance: 3.732e+04\n",
      "  model = cd_fast.enet_coordinate_descent(\n"
     ]
    }
   ],
   "source": [
    "print(f\"\\n\\n\")\n",
    "print(f\"=\"*60)\n",
    "print(f\"Part 2: Weather -> Daily Electricity Consumption Prediction\")\n",
    "print(f\"=\"*60)\n",
    "\n",
    "df2 = pd.read_csv('/home/veronica/桌面/Homework1/electricity_consumption_based_weather_dataset.csv')\n",
    "\n",
    "# clean the csv thats whymissing data\n",
    "df2 = df2.dropna()\n",
    "print(f'After dropna shape: {df2.shape}')\n",
    "\n",
    "feature_cols_2 = ['AWND', 'PRCP', 'TMAX', 'TMIN']\n",
    "X2 = df2[feature_cols_2].values\n",
    "y2 = df2['daily_consumption'].values\n",
    "\n",
    "X2_train, X2_test, y2_train, y2_test = train_test_split(X2, y2, test_size=0.3, random_state=42)\n",
    "print(f'Training: {len(X2_train)}, Testing: {len(X2_test)}')\n",
    "\n",
    "lr2 = LinearRegression()\n",
    "lr2.fit(X2_train, y2_train)\n",
    "ridge2 = Ridge(alpha=1.0)\n",
    "ridge2.fit(X2_train, y2_train)\n",
    "lasso2 = Lasso(alpha=1.0)\n",
    "lasso2.fit(X2_train, y2_train)\n",
    "\n",
    "poly2_2 = PolynomialFeatures(degree=2)\n",
    "X2_train_poly2 = poly2_2.fit_transform(X2_train)\n",
    "X2_test_poly2 = poly2_2.transform(X2_test)\n",
    "poly3_2 = PolynomialFeatures(degree=3)\n",
    "X2_train_poly3 = poly3_2.fit_transform(X2_train)\n",
    "X2_test_poly3 = poly3_2.transform(X2_test)\n",
    "\n",
    "lr_poly2_2 = LinearRegression()\n",
    "lr_poly2_2.fit(X2_train_poly2, y2_train)\n",
    "lr_poly3_2 = LinearRegression()\n",
    "lr_poly3_2.fit(X2_train_poly3, y2_train)\n",
    "ridge_poly2_2 = Ridge(alpha=1.0)\n",
    "ridge_poly2_2.fit(X2_train_poly2, y2_train)\n",
    "ridge_poly3_2 = Ridge(alpha=1.0)\n",
    "ridge_poly3_2.fit(X2_train_poly3, y2_train)\n",
    "lasso_poly2_2 = Lasso(alpha=1.0)\n",
    "lasso_poly2_2.fit(X2_train_poly2, y2_train)\n",
    "lasso_poly3_2 = Lasso(alpha=1.0)\n",
    "lasso_poly3_2.fit(X2_train_poly3, y2_train)\n",
    "\n",
    "res2 = []\n",
    "y2_train_pred=lr2.predict(X2_train)\n",
    "y2_test_pred=lr2.predict(X2_test)\n",
    "res2.append(['Linear Regression', mean_squared_error(y2_train, y2_train_pred), mean_absolute_error(y2_train, y2_train_pred), r2_score(y2_train, y2_train_pred), mean_squared_error(y2_test, y2_test_pred), mean_absolute_error(y2_test, y2_test_pred), r2_score(y2_test, y2_test_pred)])\n",
    "\n",
    "y2_train_pred=ridge2.predict(X2_train)\n",
    "y2_test_pred=ridge2.predict(X2_test)\n",
    "res2.append(['Ridge Regression', mean_squared_error(y2_train, y2_train_pred), mean_absolute_error(y2_train, y2_train_pred), r2_score(y2_train, y2_train_pred), mean_squared_error(y2_test, y2_test_pred), mean_absolute_error(y2_test, y2_test_pred), r2_score(y2_test, y2_test_pred)])\n",
    "\n",
    "y2_train_pred=lasso2.predict(X2_train)\n",
    "y2_test_pred=lasso2.predict(X2_test)\n",
    "res2.append(['Lasso Regression', mean_squared_error(y2_train, y2_train_pred), mean_absolute_error(y2_train, y2_train_pred), r2_score(y2_train, y2_train_pred), mean_squared_error(y2_test, y2_test_pred), mean_absolute_error(y2_test, y2_test_pred), r2_score(y2_test, y2_test_pred)])\n",
    "\n",
    "y2_train_pred=lr_poly2_2.predict(X2_train_poly2)\n",
    "y2_test_pred=lr_poly2_2.predict(X2_test_poly2)\n",
    "res2.append(['Poly2 + Linear', mean_squared_error(y2_train, y2_train_pred), mean_absolute_error(y2_train, y2_train_pred), r2_score(y2_train, y2_train_pred), mean_squared_error(y2_test, y2_test_pred), mean_absolute_error(y2_test, y2_test_pred), r2_score(y2_test, y2_test_pred)])\n",
    "\n",
    "y2_train_pred=lr_poly3_2.predict(X2_train_poly3)\n",
    "y2_test_pred=lr_poly3_2.predict(X2_test_poly3)\n",
    "res2.append(['Poly3 + Linear', mean_squared_error(y2_train, y2_train_pred), mean_absolute_error(y2_train, y2_train_pred), r2_score(y2_train, y2_train_pred), mean_squared_error(y2_test, y2_test_pred), mean_absolute_error(y2_test, y2_test_pred), r2_score(y2_test, y2_test_pred)])\n",
    "\n",
    "y2_train_pred=ridge_poly2_2.predict(X2_train_poly2)\n",
    "y2_test_pred=ridge_poly2_2.predict(X2_test_poly2)\n",
    "res2.append(['Poly2 + Ridge', mean_squared_error(y2_train, y2_train_pred), mean_absolute_error(y2_train, y2_train_pred), r2_score(y2_train, y2_train_pred), mean_squared_error(y2_test, y2_test_pred), mean_absolute_error(y2_test, y2_test_pred), r2_score(y2_test, y2_test_pred)])\n",
    "\n",
    "y2_train_pred=ridge_poly3_2.predict(X2_train_poly3)\n",
    "y2_test_pred=ridge_poly3_2.predict(X2_test_poly3)\n",
    "res2.append(['Poly3 + Ridge', mean_squared_error(y2_train, y2_train_pred), mean_absolute_error(y2_train, y2_train_pred), r2_score(y2_train, y2_train_pred), mean_squared_error(y2_test, y2_test_pred), mean_absolute_error(y2_test, y2_test_pred), r2_score(y2_test, y2_test_pred)])\n",
    "\n",
    "y2_train_pred=lasso_poly2_2.predict(X2_train_poly2)\n",
    "y2_test_pred=lasso_poly2_2.predict(X2_test_poly2)\n",
    "res2.append(['Poly2 + Lasso', mean_squared_error(y2_train, y2_train_pred), mean_absolute_error(y2_train, y2_train_pred), r2_score(y2_train, y2_train_pred), mean_squared_error(y2_test, y2_test_pred), mean_absolute_error(y2_test, y2_test_pred), r2_score(y2_test, y2_test_pred)])\n",
    "\n",
    "y2_train_pred=lasso_poly3_2.predict(X2_train_poly3)\n",
    "y2_test_pred=lasso_poly3_2.predict(X2_test_poly3)\n",
    "res2.append(['Poly3 + Lasso', mean_squared_error(y2_train, y2_train_pred), mean_absolute_error(y2_train, y2_train_pred), r2_score(y2_train, y2_train_pred), mean_squared_error(y2_test, y2_test_pred), mean_absolute_error(y2_test, y2_test_pred), r2_score(y2_test, y2_test_pred)])\n",
    "\n",
    "df_res2 = pd.DataFrame(res2, columns=['Model', 'Train MSE', 'Train MAE', 'Train R2', 'Test MSE', 'Test MAE', 'Test R2'])\n",
    "print(df_res2.to_string(index=False))\n",
    "\n",
    "best_idx=0\n",
    "best_r2=res2[0][6]\n",
    "i=1\n",
    "while i<len(res2):\n",
    "    if res2[i][6]>best_r2:\n",
    "        best_r2=res2[i][6]\n",
    "        best_idx=i\n",
    "    i+=1\n",
    "print(f'Best model: {res2[best_idx][0]} with test R2={best_r2:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70bee870",
   "metadata": {},
   "source": [
    "All models have very low test R2 scores (around 0.25-0.30). The best model is Lasso Regression with test R2 = 0.2993.\n",
    "\n",
    "The linear models (Linear, Ridge, Lasso) actually perform better on test data than the polynomial models. The polynomial models have higher train R2 but lower test R2, which means they are overfitting the training data.\n",
    "\n",
    "The low R2 scores (around 0.30) mean that weather features can only explain about 30% of the variance in electricity consumption. This suggests that electricity consumption depends on many other factors not in this dataset, like day of week, holidays, or human behavior. Weather alone is not a good predictor.\n",
    "\n",
    "Since linear models generalize better than polynomial models, the relationship between weather and electricity consumption seems to be mostly linear, but just very weak overall.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d4031939",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "============================================================\n",
      "Part 3: Power Plant Data -> PE Prediction\n",
      "============================================================\n",
      "\n",
      "3.1 Load and Inspect Dataset\n",
      "Column names: ['AT', 'V', 'AP', 'RH', 'PE']\n",
      "Shape: (9568, 5)\n",
      "\n",
      "Summary Statistics:\n",
      "                AT            V           AP           RH           PE\n",
      "count  9568.000000  9568.000000  9568.000000  9568.000000  9568.000000\n",
      "mean     19.651231    54.305804  1013.259078    73.308978   454.365009\n",
      "std       7.452473    12.707893     5.938784    14.600269    17.066995\n",
      "min       1.810000    25.360000   992.890000    25.560000   420.260000\n",
      "25%      13.510000    41.740000  1009.100000    63.327500   439.750000\n",
      "50%      20.345000    52.080000  1012.940000    74.975000   451.550000\n",
      "75%      25.720000    66.540000  1017.260000    84.830000   468.430000\n",
      "max      37.110000    81.560000  1033.300000   100.160000   495.760000\n",
      "\n",
      "Missing values:\n",
      "AT    0\n",
      "V     0\n",
      "AP    0\n",
      "RH    0\n",
      "PE    0\n",
      "dtype: int64\n",
      "\n",
      "Target (DV): PE (Electrical Energy Output)\n",
      "Features (IVs): AT (Ambient Temperature), V (Exhaust Vacuum), AP (Ambient Pressure), RH (Relative Humidity)\n",
      "\n",
      "Rows with missing: 0\n",
      "\n",
      "Training: 6697\n",
      "Testing: 2871\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/veronica/桌面/Homework1/.venv/lib/python3.12/site-packages/sklearn/linear_model/_ridge.py:228: LinAlgWarning: An ill-conditioned matrix detected: slice 0 has rcond = 2.598279743128814e-19.\n",
      "  return linalg.solve(A, Xy, assume_a=\"pos\", overwrite_a=True).T\n",
      "/home/veronica/桌面/Homework1/.venv/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:716: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.336e+04, tolerance: 1.945e+02\n",
      "  model = cd_fast.enet_coordinate_descent(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Models trained, alpha = 1.0\n",
      "            Model  Train MSE  Train MAE  Train R2  Test MSE  Test MAE  Test R2\n",
      "Linear Regression  20.580840   3.607182  0.929136 21.239857  3.649933 0.927548\n",
      " Ridge Regression  20.580840   3.607186  0.929136 21.239805  3.649931 0.927549\n",
      " Lasso Regression  20.647117   3.617731  0.928908 21.247793  3.649382 0.927521\n",
      "   Poly2 + Linear  17.887800   3.333026  0.938409 18.647312  3.386145 0.936392\n",
      "   Poly3 + Linear  17.079895   3.211027  0.941191 17.614004  3.264772 0.939917\n",
      "    Poly2 + Ridge  17.894772   3.333950  0.938385 18.643907  3.386515 0.936404\n",
      "    Poly3 + Ridge  17.136922   3.220419  0.940994 17.699075  3.274420 0.939626\n",
      "    Poly2 + Lasso  18.507546   3.411508  0.936275 19.215444  3.460524 0.934454\n",
      "    Poly3 + Lasso  18.260967   3.360199  0.937124 18.918335  3.409587 0.935467\n",
      "Best model: Poly3 + Linear with test R2=0.9399\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/veronica/桌面/Homework1/.venv/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:716: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.145e+04, tolerance: 1.945e+02\n",
      "  model = cd_fast.enet_coordinate_descent(\n"
     ]
    }
   ],
   "source": [
    "print(f\"\\n\\n\")\n",
    "print(f\"=\"*60)\n",
    "print(f\"Part 3: Power Plant Data -> PE Prediction\")\n",
    "print(f\"=\"*60)\n",
    "\n",
    "df3 = pd.read_csv('/home/veronica/桌面/Homework1/usina.csv')\n",
    "print(f\"\\n3.1 Load and Inspect Dataset\")\n",
    "print(f\"Column names: {list(df3.columns)}\")\n",
    "print(f\"Shape: {df3.shape}\")\n",
    "print(f\"\\nSummary Statistics:\")\n",
    "print(df3.describe())\n",
    "print(f\"\\nMissing values:\")\n",
    "print(df3.isnull().sum())\n",
    "print(f\"\\nTarget (DV): PE (Electrical Energy Output)\")\n",
    "print(f\"Features (IVs): AT (Ambient Temperature), V (Exhaust Vacuum), AP (Ambient Pressure), RH (Relative Humidity)\")\n",
    "\n",
    "# features and target\n",
    "feature_cols_3 = ['AT', 'V', 'AP', 'RH']\n",
    "X3 = df3[feature_cols_3].values\n",
    "y3 = df3['PE'].values\n",
    "\n",
    "# handle missing if any\n",
    "missing_rows = df3.isnull().any(axis=1).sum()\n",
    "print(f\"\\nRows with missing: {missing_rows}\")\n",
    "if missing_rows>0:\n",
    "    df3 = df3.dropna()\n",
    "    X3 = df3[feature_cols_3].values\n",
    "    y3 = df3['PE'].values\n",
    "    print(f\"After dropping: {df3.shape}\")\n",
    "\n",
    "# 3.2 split\n",
    "X3_train, X3_test, y3_train, y3_test = train_test_split(X3, y3, test_size=0.3, random_state=42)\n",
    "print(f\"\\nTraining: {len(X3_train)}\")\n",
    "print(f\"Testing: {len(X3_test)}\")\n",
    "\n",
    "# 3.3 train models\n",
    "\n",
    "lr3 = LinearRegression()\n",
    "lr3.fit(X3_train, y3_train)\n",
    "\n",
    "ridge3 = Ridge(alpha=1.0)\n",
    "ridge3.fit(X3_train, y3_train)\n",
    "\n",
    "lasso3 = Lasso(alpha=1.0)\n",
    "lasso3.fit(X3_train, y3_train)\n",
    "\n",
    "poly2_3 = PolynomialFeatures(degree=2)\n",
    "X3_train_poly2 = poly2_3.fit_transform(X3_train)\n",
    "X3_test_poly2 = poly2_3.transform(X3_test)\n",
    "\n",
    "poly3_3 = PolynomialFeatures(degree=3)\n",
    "X3_train_poly3 = poly3_3.fit_transform(X3_train)\n",
    "X3_test_poly3 = poly3_3.transform(X3_test)\n",
    "\n",
    "lr_poly2_3 = LinearRegression()\n",
    "lr_poly2_3.fit(X3_train_poly2, y3_train)\n",
    "\n",
    "lr_poly3_3 = LinearRegression()\n",
    "lr_poly3_3.fit(X3_train_poly3, y3_train)\n",
    "\n",
    "ridge_poly2_3 = Ridge(alpha=1.0)\n",
    "ridge_poly2_3.fit(X3_train_poly2, y3_train)\n",
    "\n",
    "ridge_poly3_3 = Ridge(alpha=1.0)\n",
    "ridge_poly3_3.fit(X3_train_poly3, y3_train)\n",
    "\n",
    "lasso_poly2_3 = Lasso(alpha=1.0)\n",
    "lasso_poly2_3.fit(X3_train_poly2, y3_train)\n",
    "\n",
    "lasso_poly3_3 = Lasso(alpha=1.0)\n",
    "lasso_poly3_3.fit(X3_train_poly3, y3_train)\n",
    "\n",
    "print(f\"Models trained, alpha = 1.0\")\n",
    "\n",
    "# 3.4 evaluation\n",
    "res3 = []\n",
    "\n",
    "y3_train_pred=lr3.predict(X3_train)\n",
    "y3_test_pred=lr3.predict(X3_test)\n",
    "res3.append(['Linear Regression', mean_squared_error(y3_train, y3_train_pred), mean_absolute_error(y3_train, y3_train_pred), r2_score(y3_train, y3_train_pred), mean_squared_error(y3_test, y3_test_pred), mean_absolute_error(y3_test, y3_test_pred), r2_score(y3_test, y3_test_pred)])\n",
    "\n",
    "y3_train_pred=ridge3.predict(X3_train)\n",
    "y3_test_pred=ridge3.predict(X3_test)\n",
    "res3.append(['Ridge Regression', mean_squared_error(y3_train, y3_train_pred), mean_absolute_error(y3_train, y3_train_pred), r2_score(y3_train, y3_train_pred), mean_squared_error(y3_test, y3_test_pred), mean_absolute_error(y3_test, y3_test_pred), r2_score(y3_test, y3_test_pred)])\n",
    "\n",
    "y3_train_pred=lasso3.predict(X3_train)\n",
    "y3_test_pred=lasso3.predict(X3_test)\n",
    "res3.append(['Lasso Regression', mean_squared_error(y3_train, y3_train_pred), mean_absolute_error(y3_train, y3_train_pred), r2_score(y3_train, y3_train_pred), mean_squared_error(y3_test, y3_test_pred), mean_absolute_error(y3_test, y3_test_pred), r2_score(y3_test, y3_test_pred)])\n",
    "\n",
    "y3_train_pred=lr_poly2_3.predict(X3_train_poly2)\n",
    "y3_test_pred=lr_poly2_3.predict(X3_test_poly2)\n",
    "res3.append(['Poly2 + Linear', mean_squared_error(y3_train, y3_train_pred), mean_absolute_error(y3_train, y3_train_pred), r2_score(y3_train, y3_train_pred), mean_squared_error(y3_test, y3_test_pred), mean_absolute_error(y3_test, y3_test_pred), r2_score(y3_test, y3_test_pred)])\n",
    "\n",
    "y3_train_pred=lr_poly3_3.predict(X3_train_poly3)\n",
    "y3_test_pred=lr_poly3_3.predict(X3_test_poly3)\n",
    "res3.append(['Poly3 + Linear', mean_squared_error(y3_train, y3_train_pred), mean_absolute_error(y3_train, y3_train_pred), r2_score(y3_train, y3_train_pred), mean_squared_error(y3_test, y3_test_pred), mean_absolute_error(y3_test, y3_test_pred), r2_score(y3_test, y3_test_pred)])\n",
    "\n",
    "y3_train_pred=ridge_poly2_3.predict(X3_train_poly2)\n",
    "y3_test_pred=ridge_poly2_3.predict(X3_test_poly2)\n",
    "res3.append(['Poly2 + Ridge', mean_squared_error(y3_train, y3_train_pred), mean_absolute_error(y3_train, y3_train_pred), r2_score(y3_train, y3_train_pred), mean_squared_error(y3_test, y3_test_pred), mean_absolute_error(y3_test, y3_test_pred), r2_score(y3_test, y3_test_pred)])\n",
    "\n",
    "y3_train_pred=ridge_poly3_3.predict(X3_train_poly3)\n",
    "y3_test_pred=ridge_poly3_3.predict(X3_test_poly3)\n",
    "res3.append(['Poly3 + Ridge', mean_squared_error(y3_train, y3_train_pred), mean_absolute_error(y3_train, y3_train_pred), r2_score(y3_train, y3_train_pred), mean_squared_error(y3_test, y3_test_pred), mean_absolute_error(y3_test, y3_test_pred), r2_score(y3_test, y3_test_pred)])\n",
    "\n",
    "y3_train_pred=lasso_poly2_3.predict(X3_train_poly2)\n",
    "y3_test_pred=lasso_poly2_3.predict(X3_test_poly2)\n",
    "res3.append(['Poly2 + Lasso', mean_squared_error(y3_train, y3_train_pred), mean_absolute_error(y3_train, y3_train_pred), r2_score(y3_train, y3_train_pred), mean_squared_error(y3_test, y3_test_pred), mean_absolute_error(y3_test, y3_test_pred), r2_score(y3_test, y3_test_pred)])\n",
    "\n",
    "y3_train_pred=lasso_poly3_3.predict(X3_train_poly3)\n",
    "y3_test_pred=lasso_poly3_3.predict(X3_test_poly3)\n",
    "res3.append(['Poly3 + Lasso', mean_squared_error(y3_train, y3_train_pred), mean_absolute_error(y3_train, y3_train_pred), r2_score(y3_train, y3_train_pred), mean_squared_error(y3_test, y3_test_pred), mean_absolute_error(y3_test, y3_test_pred), r2_score(y3_test, y3_test_pred)])\n",
    "\n",
    "df_res3 = pd.DataFrame(res3, columns=['Model', 'Train MSE', 'Train MAE', 'Train R2', 'Test MSE', 'Test MAE', 'Test R2'])\n",
    "print(df_res3.to_string(index=False))\n",
    "\n",
    "# 3.5 find best\n",
    "best_idx=0\n",
    "best_r2=res3[0][6]\n",
    "i=1\n",
    "while i<len(res3):\n",
    "    if res3[i][6]>best_r2:\n",
    "        best_r2=res3[i][6]\n",
    "        best_idx=i\n",
    "    i+=1\n",
    "print(f\"Best model: {res3[best_idx][0]} with test R2={best_r2:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd646ecc",
   "metadata": {},
   "source": [
    "Poly3 + Linear has the best test R2 = 0.9399. This is higher than basic Linear Regression which has test R2 = 0.9275.\n",
    "\n",
    "The polynomial models (especially degree 3) perform better than linear models. This suggests that PE (electrical energy output) has a nonlinear relationship with the input features (AT, V, AP, RH).\n",
    "\n",
    "Ridge Regression has almost the same performance as Linear Regression (both around 0.9275). Lasso is a little bit worse. So regularization does not help much on this dataset. I think this is because the features are not very correlated with each other, so there is no need for regularization to reduce overfitting.\n",
    "\n",
    "The best model is Poly3 + Linear because polynomial features can capture the nonlinear patterns in power plant data."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
