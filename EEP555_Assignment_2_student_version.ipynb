{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "d759c4d3",
      "metadata": {
        "id": "d759c4d3"
      },
      "source": [
        "# EE P 555 — Assignment: Power-System Fault Classification (Traditional ML + DNNs)\n",
        "\n",
        "In this assignment, you will predict **power-system fault conditions** from 6 input measurements.\n",
        "You will first implement a set of **traditional machine-learning baselines**, then implement and compare **three** PyTorch DNN formulations.\n",
        "\n",
        "**Dataset**: `classData.csv`\n",
        "\n",
        "**Inputs**: `[Ia, Ib, Ic, Va, Vb, Vc]`  \n",
        "**Outputs**: 4-bit label `[G, C, B, A]`\n",
        "\n",
        "Dataset (Kaggle): https://www.kaggle.com/datasets/esathyaprakash/electrical-fault-detection-and-classification\n",
        "\n",
        "**Note:** Do **NOT** download the dataset from Kaggle. Use the dataset file provided on Canvas with this assignment.\n",
        "\n",
        "Your tasks:\n",
        "1. **Part 1** — Traditional ML baselines for **6-class fault type** (train + test reports + confusion matrices).\n",
        "2. **Part 2** — Regression-style DNN to predict 4 outputs (then round to bits).\n",
        "3. **Part 3** — Multi-label DNN to predict 4 bits (sigmoid / BCE-style loss).\n",
        "4. **Part 4** — Multi-class DNN to predict no fault and 5 fault types (softmax / cross-entropy).\n",
        "5. **Part 5** — Brief discussion: which approach is best and why.\n",
        "6. **Part 6** — Open-ended: build a **binary** classifier (Fault vs No Fault) using the best approach.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a9f2ac0e",
      "metadata": {
        "id": "a9f2ac0e"
      },
      "source": [
        "## Setup\n",
        "Run the next cell to import libraries and define helper functions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "146513bc",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "146513bc",
        "outputId": "68039bb2-5bb7-4156-e204-480283405bdc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Device: cpu\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import RobustScaler\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Reproducibility\n",
        "RANDOM_STATE = 42\n",
        "np.random.seed(RANDOM_STATE)\n",
        "torch.manual_seed(RANDOM_STATE)\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print('Device:', device)\n",
        "\n",
        "def plot_cm(cm, title, labels):\n",
        "    plt.figure()\n",
        "    plt.imshow(cm, interpolation='nearest')\n",
        "    plt.title(title)\n",
        "    plt.colorbar()\n",
        "    ticks = np.arange(len(labels))\n",
        "    plt.xticks(ticks, labels, rotation=45, ha='right')\n",
        "    plt.yticks(ticks, labels)\n",
        "    plt.xlabel('Predicted')\n",
        "    plt.ylabel('True')\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "def bit_reports(Y_true_bits, Y_pred_bits, split_name='Test', bit_names=('G','C','B','A')):\n",
        "    \"\"\"Multi-label report + per-bit 2x2 confusion matrices.\"\"\"\n",
        "    print(f\"\\n=== Bit-level Report | {split_name} ===\")\n",
        "    print(classification_report(Y_true_bits, Y_pred_bits, target_names=list(bit_names), digits=4))\n",
        "    for j, lab in enumerate(bit_names):\n",
        "        cm = confusion_matrix(Y_true_bits[:, j], Y_pred_bits[:, j], labels=[0, 1])\n",
        "        print(f\"\\nBit {lab} | {split_name} CM (rows=true 0/1, cols=pred 0/1):\\n{cm}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "df53417d",
      "metadata": {
        "id": "df53417d"
      },
      "source": [
        "## Dataset\n",
        "\n",
        "The file `classData.csv` contains a dataset for classifying fault conditions.\n",
        "\n",
        "- **Inputs**: `[Ia, Ib, Ic, Va, Vb, Vc]`\n",
        "- **Outputs**: `[G, C, B, A]`\n",
        "\n",
        "Examples:\n",
        "- `[0 0 0 0]` — No Fault\n",
        "- `[1 0 0 1]` — LG fault (Phase A to ground)\n",
        "- `[0 0 1 1]` — LL fault (Phase A to Phase B)\n",
        "- `[1 0 1 1]` — LLG fault (Phases A,B to ground)\n",
        "- `[0 1 1 1]` — LLL fault (all three phases)\n",
        "- `[1 1 1 1]` — LLLG fault (three phases + ground)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "629b0a90",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "629b0a90",
        "outputId": "8d7610da-97ac-4b01-c86a-414a117617d9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Fault counts: {'No Fault': np.int64(2365), 'LG': np.int64(1129), 'LL': np.int64(1004), 'LLG': np.int64(1134), 'LLL': np.int64(1096), 'LLLG': np.int64(1133)}\n",
            "Train size: 5502 | Test size: 2359\n"
          ]
        }
      ],
      "source": [
        "# ----------------------------\n",
        "# Load data + create targets\n",
        "# ----------------------------\n",
        "CSV_PATH = 'classData.csv'\n",
        "df = pd.read_csv(CSV_PATH)\n",
        "\n",
        "label_cols = ['G','C','B','A']\n",
        "feature_cols = ['Ia','Ib','Ic','Va','Vb','Vc']\n",
        "\n",
        "missing = [c for c in (label_cols + feature_cols) if c not in df.columns]\n",
        "if missing:\n",
        "    raise ValueError(f\"Missing expected columns: {missing}. Found: {list(df.columns)}\")\n",
        "\n",
        "X = df[feature_cols].astype(np.float32).values            # (N,6)\n",
        "Y_bits = df[label_cols].astype(int).values                # (N,4)\n",
        "\n",
        "FAULT_NAMES = ['No Fault','LG','LL','LLG','LLL','LLLG']\n",
        "\n",
        "def bits_to_fault_type_idx(bits_gcba):\n",
        "    G, C, B, A = map(int, bits_gcba)\n",
        "    n_phase = C + B + A\n",
        "    if G == 0 and n_phase == 0: return 0\n",
        "    if G == 1 and n_phase == 1: return 1\n",
        "    if G == 0 and n_phase == 2: return 2\n",
        "    if G == 1 and n_phase == 2: return 3\n",
        "    if G == 0 and n_phase == 3: return 4\n",
        "    if G == 1 and n_phase == 3: return 5\n",
        "    raise ValueError(f\"Unexpected pattern [G,C,B,A]={bits_gcba}\")\n",
        "\n",
        "y_fault = np.array([bits_to_fault_type_idx(row) for row in Y_bits], dtype=int)\n",
        "print('Fault counts:', dict(zip(FAULT_NAMES, np.bincount(y_fault, minlength=len(FAULT_NAMES)))))\n",
        "\n",
        "# One shared split (recommended for fair comparison across Parts 1–3)\n",
        "X_train, X_test, Y_train, Y_test, y_fault_train, y_fault_test = train_test_split(\n",
        "    X, Y_bits, y_fault, test_size=0.30, random_state=RANDOM_STATE, stratify=y_fault\n",
        ")\n",
        "print('Train size:', X_train.shape[0], '| Test size:', X_test.shape[0])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "28109fa9",
      "metadata": {
        "id": "28109fa9"
      },
      "source": [
        "## Part 1 — Traditional ML baselines (6-class fault type)\n",
        "\n",
        "In this part, you will train and evaluate several **traditional ML models** for **multi-class fault type classification**.\n",
        "\n",
        "Requirements:\n",
        "- Use a **70% / 30%** train/test split (**stratified**).\n",
        "- Use **RobustScaler** on features (**fit on train only**).\n",
        "- Train the following models:\n",
        "  - Logistic Regression\n",
        "  - Perceptron\n",
        "  - Linear SVM\n",
        "  - Gaussian Naive Bayes\n",
        "  - KNN\n",
        "  - Decision Tree\n",
        "  - Random Forest\n",
        "- For **both train and test**, print:\n",
        "  - `classification_report(...)`\n",
        "  - `confusion_matrix(...)`\n",
        "\n",
        "Notes:\n",
        "- Do **not** plot confusion matrices in this part (print only).\n",
        "- Use the provided mapping from 4-bit labels `[G, C, B, A]` to 6 fault types.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b76f647a",
      "metadata": {
        "id": "b76f647a"
      },
      "outputs": [],
      "source": [
        "# ----------------------------\n",
        "# Load data + map 4-bit labels -> 6-class fault type\n",
        "# ----------------------------\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import RobustScaler\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "\n",
        "from sklearn.linear_model import LogisticRegression, Perceptron\n",
        "from sklearn.svm import LinearSVC\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "CSV_PATH = \"classData.csv\"\n",
        "df = pd.read_csv(CSV_PATH)\n",
        "\n",
        "label_cols = [\"G\", \"C\", \"B\", \"A\"]\n",
        "feature_cols = [\"Ia\", \"Ib\", \"Ic\", \"Va\", \"Vb\", \"Vc\"]\n",
        "\n",
        "X = df[feature_cols].astype(np.float32).values\n",
        "Y_bits = df[label_cols].astype(int).values  # (N,4) in order [G,C,B,A]\n",
        "\n",
        "FAULT_NAMES = [\"No Fault\", \"LG\", \"LL\", \"LLG\", \"LLL\", \"LLLG\"]\n",
        "\n",
        "def bits_to_fault_type_idx(bits_gcba):\n",
        "    G, C, B, A = map(int, bits_gcba)\n",
        "    n_phase = C + B + A\n",
        "    if G == 0 and n_phase == 0: return 0  # No Fault\n",
        "    if G == 1 and n_phase == 1: return 1  # LG\n",
        "    if G == 0 and n_phase == 2: return 2  # LL\n",
        "    if G == 1 and n_phase == 2: return 3  # LLG\n",
        "    if G == 0 and n_phase == 3: return 4  # LLL\n",
        "    if G == 1 and n_phase == 3: return 5  # LLLG\n",
        "    raise ValueError(f\"Unexpected pattern [G,C,B,A]={bits_gcba}\")\n",
        "\n",
        "y_fault = np.array([bits_to_fault_type_idx(row) for row in Y_bits], dtype=int)\n",
        "print(\"Fault counts:\", dict(zip(FAULT_NAMES, np.bincount(y_fault, minlength=len(FAULT_NAMES)))))\n",
        "\n",
        "# ----------------------------\n",
        "# 70/30 split (stratified by fault type)\n",
        "# ----------------------------\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y_fault, test_size=0.30, random_state=42, stratify=y_fault\n",
        ")\n",
        "\n",
        "# ----------------------------\n",
        "# Robust scaling (fit on train only)\n",
        "# ----------------------------\n",
        "scaler = RobustScaler()\n",
        "X_train_s = scaler.fit_transform(X_train)\n",
        "X_test_s  = scaler.transform(X_test)\n",
        "\n",
        "# ----------------------------\n",
        "# Reasonable hyperparameter defaults (you may adjust if needed)\n",
        "# ----------------------------\n",
        "DT_MAX_DEPTH = 10\n",
        "DT_MIN_SAMPLES_LEAF = 5\n",
        "DT_MIN_SAMPLES_SPLIT = 10\n",
        "\n",
        "RF_N_ESTIMATORS = 100\n",
        "RF_MAX_DEPTH = 12\n",
        "RF_MIN_SAMPLES_LEAF = 3\n",
        "RF_MIN_SAMPLES_SPLIT = 8\n",
        "RF_MAX_FEATURES = \"sqrt\"\n",
        "\n",
        "KNN_K = 10\n",
        "\n",
        "# ----------------------------\n",
        "# TODO 1: Define the models (use the defaults above as a starting point)\n",
        "# ----------------------------\n",
        "models = {\n",
        "    # \"LogReg\": LogisticRegression(...),\n",
        "    # \"Perceptron\": Perceptron(...),\n",
        "    # \"LinearSVM\": LinearSVC(...),\n",
        "    # \"GaussianNB\": GaussianNB(...),\n",
        "    # \"KNN\": KNeighborsClassifier(...),\n",
        "    # \"DecisionTree\": DecisionTreeClassifier(...),\n",
        "    # \"RandomForest\": RandomForestClassifier(...),\n",
        "}\n",
        "\n",
        "# ----------------------------\n",
        "# Evaluate each model (train + test)\n",
        "# ----------------------------\n",
        "for name, clf in models.items():\n",
        "    clf.fit(X_train_s, y_train)\n",
        "\n",
        "    for split_name, Xs, ys in [(\"Train\", X_train_s, y_train), (\"Test\", X_test_s, y_test)]:\n",
        "        preds = clf.predict(Xs)\n",
        "        print(f\"\\n=== {name} | {split_name} ===\")\n",
        "        print(classification_report(ys, preds, target_names=FAULT_NAMES, digits=4))\n",
        "        print(\"Confusion matrix:\")\n",
        "        print(confusion_matrix(ys, preds, labels=np.arange(len(FAULT_NAMES))))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3cfb4d55",
      "metadata": {
        "id": "3cfb4d55"
      },
      "source": [
        "## Part 2 — DNN Model 1 (Regression-style 4-output model)\n",
        "\n",
        "You will build a DNN that outputs **4 real-valued numbers** and train it with **MSE**.\n",
        "During evaluation, you will **round** predictions to the nearest integer, clamp to `{0,1}`, and then compare against the true 4-bit label.\n",
        "\n",
        "Model structure:\n",
        "- Input: 6 features\n",
        "- Hidden: 16 → 8 (ReLU)\n",
        "- Output: 4 (linear)\n",
        "- Include: **BatchNorm**, **Dropout**, and **L2 regularization**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "92a85ded",
      "metadata": {
        "id": "92a85ded"
      },
      "outputs": [],
      "source": [
        "# ----------------------------\n",
        "# Part 1: Robust scaling (fit on train only)\n",
        "# ----------------------------\n",
        "scaler_1 = RobustScaler()\n",
        "X_train_s1 = scaler_1.fit_transform(X_train).astype(np.float32)\n",
        "X_test_s1  = scaler_1.transform(X_test).astype(np.float32)\n",
        "\n",
        "# Torch datasets\n",
        "Xtr = torch.from_numpy(X_train_s1)\n",
        "Xte = torch.from_numpy(X_test_s1)\n",
        "Ytr = torch.from_numpy(Y_train.astype(np.float32))\n",
        "Yte = torch.from_numpy(Y_test.astype(np.float32))\n",
        "\n",
        "BATCH_SIZE = 256\n",
        "train_loader_1 = DataLoader(TensorDataset(Xtr, Ytr), batch_size=BATCH_SIZE, shuffle=True)\n",
        "\n",
        "# Hyperparameters\n",
        "INPUT_DIM = 6\n",
        "H1 = 16\n",
        "H2 = 8\n",
        "OUT_DIM = 4\n",
        "DROPOUT_P = 0.25\n",
        "L2_WEIGHT_DECAY = 1e-4\n",
        "LR = 1e-3\n",
        "EPOCHS = 1000\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ef6a8e86",
      "metadata": {
        "id": "ef6a8e86"
      },
      "outputs": [],
      "source": [
        "## [ TODO 1 ]\n",
        "# Define the DNN for Part 1 using PyTorch.\n",
        "# Requirements:\n",
        "# - Architecture: 6 -> 16 -> 8 -> 4\n",
        "# - Hidden activations: ReLU\n",
        "# - Output: linear (no activation)\n",
        "# - Use BatchNorm and Dropout in hidden layers\n",
        "#\n",
        "# Write your model class below.\n",
        "\n",
        "class DNN_Reg4(nn.Module):\n",
        "    def __init__(self, input_dim=INPUT_DIM, h1=H1, h2=H2, out_dim=OUT_DIM, p_drop=DROPOUT_P):\n",
        "        super().__init__()\n",
        "        # TODO: define layers (Linear, BatchNorm1d, Dropout, etc.)\n",
        "        # self....\n",
        "        pass\n",
        "\n",
        "    def forward(self, x):\n",
        "        # TODO: implement forward pass\n",
        "        pass\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "66967d3f",
      "metadata": {
        "id": "66967d3f"
      },
      "outputs": [],
      "source": [
        "## [ TODO 2 ]\n",
        "# Create the model, loss function, and optimizer.\n",
        "# Requirements:\n",
        "# - Loss: MSE\n",
        "# - Optimizer: Adam\n",
        "# - Include L2 regularization using weight_decay\n",
        "\n",
        "# TODO: model = ...\n",
        "# TODO: loss_fn = ...\n",
        "# TODO: optimizer = ...\n",
        "\n",
        "# model = DNN_Reg4(...).to(device)\n",
        "# loss_fn = ...\n",
        "# optimizer = ...\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cd59e4df",
      "metadata": {
        "id": "cd59e4df"
      },
      "outputs": [],
      "source": [
        "# ----------------------------\n",
        "# Training loop (you fill the key missing pieces)\n",
        "# ----------------------------\n",
        "## [ TODO 3 ]\n",
        "def train_part1(model, loader, epochs=EPOCHS):\n",
        "    model.train()\n",
        "    for ep in range(1, epochs + 1):\n",
        "        total = 0.0\n",
        "        for xb, yb in loader:\n",
        "            xb = xb.to(device)\n",
        "            yb = yb.to(device)\n",
        "\n",
        "            # TODO: zero gradients\n",
        "            # TODO: forward pass\n",
        "            # TODO: compute loss\n",
        "            # TODO: backward\n",
        "            # TODO: optimizer step\n",
        "\n",
        "            # total += loss.item() * xb.size(0)\n",
        "            pass\n",
        "\n",
        "        if ep in [1, epochs//2, epochs]:\n",
        "            # TODO: print epoch + average loss\n",
        "            pass\n",
        "\n",
        "@torch.no_grad()\n",
        "def predict_part1(model, X_np):\n",
        "    model.eval()\n",
        "    X_t = torch.from_numpy(X_np.astype(np.float32)).to(device)\n",
        "    out = model(X_t).cpu().numpy()  # (N,4) real-valued\n",
        "    return out\n",
        "\n",
        "# TODO: Train the model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f9127eeb",
      "metadata": {
        "id": "f9127eeb"
      },
      "outputs": [],
      "source": [
        "# ----------------------------\n",
        "# Evaluation (round -> clamp to {0,1} -> report)\n",
        "# ----------------------------\n",
        "yhat_train = predict_part1(model, X_train_s1)\n",
        "yhat_test  = predict_part1(model, X_test_s1)\n",
        "\n",
        "# After you obtain yhat_* (shape N x 4):\n",
        "pred_bits_train = np.clip(np.rint(yhat_train).astype(int), 0, 1)\n",
        "pred_bits_test  = np.clip(np.rint(yhat_test ).astype(int), 0, 1)\n",
        "bit_reports(Y_train, pred_bits_train, split_name='Train', bit_names=label_cols)\n",
        "bit_reports(Y_test,  pred_bits_test,  split_name='Test',  bit_names=label_cols)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "MhPe4nnbiZEW",
      "metadata": {
        "id": "MhPe4nnbiZEW"
      },
      "outputs": [],
      "source": [
        "# One confusion matrix over full 4-bit patterns (for TEST set)\n",
        "true_str = [''.join(map(str, r.tolist())) for r in Y_test]\n",
        "pred_str = [''.join(map(str, r.tolist())) for r in pred_bits_test]\n",
        "labels = sorted(list(set(true_str) | set(pred_str)))\n",
        "cm = confusion_matrix(true_str, pred_str, labels=labels)\n",
        "plot_cm(cm, 'Part 1: 4-bit Pattern Confusion Matrix (Test)', labels)\n",
        "print(cm)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9ffcd7f7",
      "metadata": {
        "id": "9ffcd7f7"
      },
      "source": [
        "## Part 3 — DNN Model 2 (Multi-label 4-bit classification)\n",
        "\n",
        "You will build a DNN that predicts the 4 bits `[G, C, B, A]` as a **multi-label** output.\n",
        "Each output dimension is an independent binary label.\n",
        "\n",
        "Model structure:\n",
        "- Input: 6 features\n",
        "- Hidden: 16 → 8 (ReLU)\n",
        "- Output: 4 logits (apply sigmoid only for evaluation)\n",
        "- Include: **BatchNorm**, **Dropout**, and **L2 regularization**\n",
        "\n",
        "Training:\n",
        "- Use a **binary cross-entropy style loss on logits** (do not apply sigmoid inside the model).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ece42483",
      "metadata": {
        "id": "ece42483"
      },
      "outputs": [],
      "source": [
        "# ----------------------------\n",
        "# Part 2: Robust scaling (fit on train only)\n",
        "# ----------------------------\n",
        "scaler_2 = RobustScaler()\n",
        "X_train_s2 = scaler_2.fit_transform(X_train).astype(np.float32)\n",
        "X_test_s2  = scaler_2.transform(X_test).astype(np.float32)\n",
        "\n",
        "Xtr2 = torch.from_numpy(X_train_s2)\n",
        "Xte2 = torch.from_numpy(X_test_s2)\n",
        "Ytr2 = torch.from_numpy(Y_train.astype(np.float32))\n",
        "Yte2 = torch.from_numpy(Y_test.astype(np.float32))\n",
        "\n",
        "train_loader_2 = DataLoader(TensorDataset(Xtr2, Ytr2), batch_size=BATCH_SIZE, shuffle=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a59a0032",
      "metadata": {
        "id": "a59a0032"
      },
      "outputs": [],
      "source": [
        "## [ TODO 1 ]\n",
        "# Define the DNN for Part 2.\n",
        "# Requirements:\n",
        "# - Architecture: 6 -> 16 -> 8 -> 4\n",
        "# - Hidden activations: ReLU\n",
        "# - Output: 4 logits (no sigmoid inside the model)\n",
        "# - Use BatchNorm and Dropout in hidden layers\n",
        "\n",
        "class DNN_MultiLabel4(nn.Module):\n",
        "    def __init__(self, input_dim=INPUT_DIM, h1=H1, h2=H2, out_dim=OUT_DIM, p_drop=DROPOUT_P):\n",
        "        super().__init__()\n",
        "        # TODO: define layers\n",
        "        pass\n",
        "\n",
        "    def forward(self, x):\n",
        "        # TODO: forward pass returning logits (N,4)\n",
        "        pass\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ce40f02b",
      "metadata": {
        "id": "ce40f02b"
      },
      "outputs": [],
      "source": [
        "## [ TODO 2 ]\n",
        "# Create the model, loss function, and optimizer.\n",
        "# Requirements:\n",
        "# - Loss: binary cross-entropy style loss on logits -- BCEWithLogitsLoss\n",
        "# - Optimizer: Adam\n",
        "# - Include L2 regularization using weight_decay\n",
        "\n",
        "# TODO: model = ...\n",
        "# TODO: loss_fn = ...\n",
        "# TODO: optimizer = ...\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8cc5f357",
      "metadata": {
        "id": "8cc5f357"
      },
      "outputs": [],
      "source": [
        "# ----------------------------\n",
        "# Training loop scaffold\n",
        "# ----------------------------\n",
        "## [ TODO 3]\n",
        "def train_part2(model, loader, epochs=EPOCHS):\n",
        "    model.train()\n",
        "    for ep in range(1, epochs + 1):\n",
        "        total = 0.0\n",
        "        for xb, yb in loader:\n",
        "            xb = xb.to(device)\n",
        "            yb = yb.to(device)\n",
        "\n",
        "            # TODO: zero gradients\n",
        "            # TODO: logits = model(xb)\n",
        "            # TODO: loss = loss_fn(logits, yb)\n",
        "            # TODO: backward + step\n",
        "\n",
        "            # total += loss.item() * xb.size(0)\n",
        "            pass\n",
        "\n",
        "        if ep in [1, epochs//2, epochs]:\n",
        "            # TODO: print epoch + average loss\n",
        "            pass\n",
        "\n",
        "@torch.no_grad()\n",
        "def predict_bits_part2(model, X_np, threshold=0.5):\n",
        "    model.eval()\n",
        "    X_t = torch.from_numpy(X_np.astype(np.float32)).to(device)\n",
        "    logits = model(X_t)\n",
        "    probs = torch.sigmoid(logits).cpu().numpy()\n",
        "    pred_bits = (probs >= threshold).astype(int)\n",
        "    return pred_bits\n",
        "\n",
        "# TODO: Train the model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0cab8bc7",
      "metadata": {
        "id": "0cab8bc7"
      },
      "outputs": [],
      "source": [
        "# ----------------------------\n",
        "# Evaluation\n",
        "# ----------------------------\n",
        "pred_bits_train_ = predict_bits_part2(model, X_train_s2)\n",
        "pred_bits_test_  = predict_bits_part2(model, X_test_s2)\n",
        "\n",
        "bit_reports(Y_train, pred_bits_train_, split_name='Train', bit_names=label_cols)\n",
        "bit_reports(Y_test,  pred_bits_test_,  split_name='Test',  bit_names=label_cols)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e9DHgf3gjZQh",
      "metadata": {
        "id": "e9DHgf3gjZQh"
      },
      "outputs": [],
      "source": [
        "# One confusion matrix over full 4-bit patterns (For TEST set)\n",
        "true_str_ = [''.join(map(str, r.tolist())) for r in Y_test]\n",
        "pred_str_ = [''.join(map(str, r.tolist())) for r in pred_bits_test_]\n",
        "labels_ = sorted(list(set(true_str_) | set(pred_str_)))\n",
        "cm_ = confusion_matrix(true_str_, pred_str_, labels=labels_)\n",
        "plot_cm(cm_, 'Part 2: 4-bit Pattern Confusion Matrix (Test)', labels_)\n",
        "print(cm_)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "366a210f",
      "metadata": {
        "id": "366a210f"
      },
      "source": [
        "## Part 4 — DNN Model 3 (Multi-class fault type classification)\n",
        "\n",
        "In this part, you will predict one of **6 fault types**.\n",
        "The 4-bit label is converted to a single class in `{0,1,2,3,4,5}` using the provided mapping.\n",
        "\n",
        "Model structure:\n",
        "- Input: 6 features\n",
        "- Hidden: 16 → 8 (ReLU)\n",
        "- Output: 6 logits\n",
        "- Include: **BatchNorm**, **Dropout**, and **L2 regularization**\n",
        "\n",
        "Training:\n",
        "- Use **cross-entropy loss** for multi-class classification.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6cc76d28",
      "metadata": {
        "id": "6cc76d28"
      },
      "outputs": [],
      "source": [
        "# ----------------------------\n",
        "# Part 3: Robust scaling (fit on train only)\n",
        "# ----------------------------\n",
        "scaler_3 = RobustScaler()\n",
        "X_train_s3 = scaler_3.fit_transform(X_train).astype(np.float32)\n",
        "X_test_s3  = scaler_3.transform(X_test).astype(np.float32)\n",
        "\n",
        "ytr3 = torch.from_numpy(y_fault_train.astype(np.int64))\n",
        "yte3 = torch.from_numpy(y_fault_test.astype(np.int64))\n",
        "Xtr3 = torch.from_numpy(X_train_s3)\n",
        "Xte3 = torch.from_numpy(X_test_s3)\n",
        "\n",
        "train_loader_3 = DataLoader(TensorDataset(Xtr3, ytr3), batch_size=BATCH_SIZE, shuffle=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "799a953b",
      "metadata": {
        "id": "799a953b"
      },
      "outputs": [],
      "source": [
        "## [ TODO 1 ]\n",
        "# Define the DNN for Part 3.\n",
        "# Requirements:\n",
        "# - Architecture: 6 -> 16 -> 8 -> 6\n",
        "# - Hidden activations: ReLU\n",
        "# - Output: 6 logits\n",
        "# - Use BatchNorm and Dropout in hidden layers\n",
        "\n",
        "class DNN_MultiClass6(nn.Module):\n",
        "    def __init__(self, input_dim=INPUT_DIM, h1=H1, h2=H2, num_classes=6, p_drop=DROPOUT_P):\n",
        "        super().__init__()\n",
        "        # TODO: define layers\n",
        "        pass\n",
        "\n",
        "    def forward(self, x):\n",
        "        # TODO: forward pass returning logits (N,6)\n",
        "        pass\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3597d64c",
      "metadata": {
        "id": "3597d64c"
      },
      "outputs": [],
      "source": [
        "## [ TODO 2 ]\n",
        "# Create the model, loss function, and optimizer.\n",
        "# Requirements:\n",
        "# - Loss: CrossEntropyLoss\n",
        "# - Optimizer: Adam\n",
        "# - Include L2 regularization using weight_decay\n",
        "\n",
        "# TODO: model = ...\n",
        "# TODO: loss_fn = ...\n",
        "# TODO: optimizer = ...\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "56f925f6",
      "metadata": {
        "id": "56f925f6"
      },
      "outputs": [],
      "source": [
        "# ----------------------------\n",
        "# Training loop scaffold\n",
        "# ----------------------------\n",
        "## [ TODO 3 ]\n",
        "def train_part3(model, loader, epochs=EPOCHS):\n",
        "    model.train()\n",
        "    for ep in range(1, epochs + 1):\n",
        "        total = 0.0\n",
        "        for xb, yb in loader:\n",
        "            xb = xb.to(device)\n",
        "            yb = yb.to(device)\n",
        "\n",
        "            # TODO: zero gradients\n",
        "            # TODO: logits = model(xb)\n",
        "            # TODO: loss = loss_fn(logits, yb)\n",
        "            # TODO: backward + step\n",
        "\n",
        "            # total += loss.item() * xb.size(0)\n",
        "            pass\n",
        "\n",
        "        if ep in [1, epochs//2, epochs]:\n",
        "            # TODO: print epoch + average loss\n",
        "            pass\n",
        "\n",
        "@torch.no_grad()\n",
        "def predict_part3(model, X_np):\n",
        "    model.eval()\n",
        "    X_t = torch.from_numpy(X_np.astype(np.float32)).to(device)\n",
        "    logits = model(X_t)\n",
        "    return torch.argmax(logits, dim=1).cpu().numpy()\n",
        "\n",
        "# TODO: Train the model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0893a6fd",
      "metadata": {
        "id": "0893a6fd"
      },
      "outputs": [],
      "source": [
        "# ----------------------------\n",
        "# Evaluation\n",
        "# ----------------------------\n",
        "pred_train = predict_part3(model, X_train_s3)\n",
        "pred_test  = predict_part3(model, X_test_s3)\n",
        "\n",
        "print('\\n=== Part 3 | Train ===')\n",
        "print(classification_report(y_fault_train, pred_train, target_names=FAULT_NAMES, digits=4))\n",
        "cm_tr = confusion_matrix(y_fault_train, pred_train, labels=np.arange(len(FAULT_NAMES)))\n",
        "plot_cm(cm_tr, 'Part 3: Confusion Matrix (Train)', FAULT_NAMES)\n",
        "\n",
        "print('\\n=== Part 3 | Test ===')\n",
        "print(classification_report(y_fault_test, pred_test, target_names=FAULT_NAMES, digits=4))\n",
        "cm_te = confusion_matrix(y_fault_test, pred_test, labels=np.arange(len(FAULT_NAMES)))\n",
        "plot_cm(cm_te, 'Part 3: Confusion Matrix (Test)', FAULT_NAMES)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "036f6116",
      "metadata": {
        "id": "036f6116"
      },
      "source": [
        "## Part 5 — Discussion\n",
        "\n",
        "Answer **2–5 sentences** for each prompt.\n",
        "\n",
        "1. Compare Parts 2–4. Which formulation fits this dataset best (regression-style, multi-label, or multi-class), and why?\n",
        "2. When you used the bit-based approaches (Part 2 and Part 3), did you observe any **inconsistent 4-bit outputs** (invalid fault patterns)? What does that suggest?\n",
        "3. Looking at confusion matrices, which fault types (or bit positions) were the hardest to predict? Give one plausible reason.\n",
        "\n",
        "### Your answers\n",
        "- Q1:\n",
        "- Q2:\n",
        "- Q3:\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "251ffd10",
      "metadata": {
        "id": "251ffd10"
      },
      "source": [
        "## Part 6 — Open-ended extension: Binary classification (Fault vs No Fault)\n",
        "\n",
        "In this part, you will create a **binary** classification problem:\n",
        "\n",
        "- Class 0: **No Fault** (`[0,0,0,0]`)\n",
        "- Class 1: **Fault** (any other pattern)\n",
        "\n",
        "**Your task**: Using the best formulation you identified in Part 5, build a DNN to classify **Fault vs No Fault**.\n",
        "\n",
        "Requirements:\n",
        "- Use a **70% / 30%** train/test split (stratified)\n",
        "- Use **RobustScaler** (fit on train only)\n",
        "- Report **classification report** and **confusion matrix** on **train and test**\n",
        "\n",
        "Implementation notes:\n",
        "- First create a dataset with two classes by compressing all fault types into a single “Fault” class (and keeping “No Fault” as the other class).\n",
        "- Keep your solution clean and focused: data, model, training, evaluation.\n",
        "\n",
        "\n",
        "You must write this part **from scratch** (no starter code is provided).\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}